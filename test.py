import re
import numpy as np
from gensim.models import Word2Vec
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# ==============================================
# 2Ô∏è‚É£ D·ªØ li·ªáu v√≠ d·ª•
# ==============================================
comments = [
    "S·∫£n ph·∫©m r·∫•t t·ªët", 
    "Giao h√†ng nhanh v√† ƒë√≥ng g√≥i c·∫©n th·∫≠n", 
    "Ch·∫•t l∆∞·ª£ng k√©m, kh√¥ng ƒë√°ng ti·ªÅn", 
    "H√†ng qu√° t·ªá, t√¥i th·∫•t v·ªçng", 
    "Shop ph·ª•c v·ª• nhi·ªát t√¨nh", 
    "Gi√° r·∫ª m√† d√πng b·ªÅn", 
    "Kh√¥ng nh∆∞ m√¥ t·∫£, r·∫•t th·∫•t v·ªçng"
]
labels = [1, 1, 0, 0, 1, 1, 0]  # 1: t√≠ch c·ª±c, 0: ti√™u c·ª±c

# ==============================================
# 3Ô∏è‚É£ L√†m s·∫°ch & Tokenization
# ==============================================
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z√†-·ªπ0-9\s]', '', text)
    return text.split()

tokenized_comments = [clean_text(c) for c in comments]

print("üëâ Sau khi tokenization:")
for i, c in enumerate(tokenized_comments):
    print(f"{i+1}. {c}")

# # ==============================================
# # 4Ô∏è‚É£ Hu·∫•n luy·ªán Word2Vec
# # ==============================================
# w2v_model = Word2Vec(sentences=tokenized_comments, vector_size=100, window=5, min_count=1, sg=1)

# print("\nüëâ V√≠ d·ª• vector Word2Vec c·ªßa t·ª´ 't·ªët':")
# print(w2v_model.wv['t·ªët'][:10])  # in 10 gi√° tr·ªã ƒë·∫ßu

# # ==============================================
# # 5Ô∏è‚É£ T·∫°o tokenizer v√† embedding matrix
# # ==============================================
# tokenizer = Tokenizer()
# tokenizer.fit_on_texts([' '.join(c) for c in tokenized_comments])
# vocab_size = len(tokenizer.word_index) + 1
# embedding_dim = 100

# embedding_matrix = np.zeros((vocab_size, embedding_dim))
# for word, i in tokenizer.word_index.items():
#     if word in w2v_model.wv:
#         embedding_matrix[i] = w2v_model.wv[word]

# print("\nüëâ S·ªë l∆∞·ª£ng t·ª´ trong vocabulary:", vocab_size)
# print("M·ªôt v√†i t·ª´ trong t·ª´ ƒëi·ªÉn:", list(tokenizer.word_index.items())[:10])